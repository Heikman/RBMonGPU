#pragma once

#include "types.h"

namespace rbm_on_gpu {


HDINLINE void tree_sum(complex_t& result, const unsigned int size, complex_t& summand) {
    #ifdef __CUDA_ARCH__

    constexpr auto warp_size = 32u;
    constexpr auto full_mask = 0xffffffff;

    if(threadIdx.x == 0) {
        result = complex_t(0.0f, 0.0f);
    }
    __syncthreads();

    unsigned mask = __ballot_sync(full_mask, threadIdx.x < size);

    if(threadIdx.x < size) {
        for(int offset = 16; offset > 0; offset /= 2) {
            summand.__re_ += __shfl_down_sync(mask, summand.real(), offset);
            summand.__im_ += __shfl_down_sync(mask, summand.imag(), offset);
        }

        if(threadIdx.x % warp_size == 0) {
            atomicAdd(&result, summand);
        }
    }

    __syncthreads();

    #else

    result += summand;

    #endif;
}

HDINLINE void tree_sum(float& result, const unsigned int size, float& summand) {
    #ifdef __CUDA_ARCH__

    constexpr auto warp_size = 32u;
    constexpr auto full_mask = 0xffffffff;

    if(threadIdx.x == 0) {
        result = 0.0f;
    }
    __syncthreads();

    unsigned mask = __ballot_sync(full_mask, threadIdx.x < size);

    if(threadIdx.x < size) {
        for(int offset = 16; offset > 0; offset /= 2) {
            summand += __shfl_down_sync(mask, summand, offset);
        }

        if(threadIdx.x % warp_size == 0) {
            atomicAdd(&result, summand);
        }
    }

    __syncthreads();

    #else

    result += summand;

    #endif
}

} // namespace rbm_on_gpu
